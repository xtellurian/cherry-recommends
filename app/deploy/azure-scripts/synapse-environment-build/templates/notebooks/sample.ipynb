{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook\n",
    "\n",
    "Copy this notebook as a starting point for analysis.\n",
    "\n",
    "Every 12 hours, the TrackedUserEvents tables of every tenant are copied over to the datalake.\n",
    "\n",
    "You can get started on backend analysis here as long as you know the tenant name and the account name (both easy to find out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name=\"\" # data lake account name\n",
    "container_name=\"tenants-sql-landing\"\n",
    "tenant=\"tenant name\"\n",
    "table=\"TrackedUserEvents\"\n",
    "file_format=\"parquet\" \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "relative_path = f'{tenant}/{table}.parquet'\n",
    "\n",
    "if account_name == \"\":\n",
    "    raise Exception(\"account_name is required\")\n",
    "\n",
    "adls_path = f'abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a spark data frame\n",
    "\n",
    "The file must be parquet for this method to work.\n",
    "There are also avro files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(adls_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = df.toPandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
