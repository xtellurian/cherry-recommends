{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3cd8e3",
   "metadata": {},
   "source": [
    "# Deploying a Model using Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b3b31",
   "metadata": {},
   "source": [
    "The steps to deploy any model are:\n",
    "\n",
    "1. Register the model\n",
    "2. Prepare an entry script\n",
    "3. Prepare an inference configuration and a deployment configuration\n",
    "4. Deploy the model locally to ensure everything works\n",
    "5. Choose a compute target.\n",
    "6. Re-deploy the model to the cloud\n",
    "7. Test the resulting web service.\n",
    "\n",
    "\n",
    "[You can learn more by reading these official docs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6f481",
   "metadata": {},
   "source": [
    "## 0. Connect to AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "03f048f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to workspace: data-workspace05a7960\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Model, Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "subscription_id = '651f2ed5-6e2f-41d0-b533-0cd28801ef2a'\n",
    "resource_group = 'data'\n",
    "workspace_name = 'data-workspace05a7960'\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "print('Connected to workspace:', ws.name)\n",
    "\n",
    "tags = {\n",
    "    'source': 'tutorial',\n",
    "    'production': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84a478",
   "metadata": {},
   "source": [
    "## 1. Register a model\n",
    "\n",
    "A typical situation for a deployed machine learning service is that you need the following components:\n",
    "\n",
    "* resources representing the specific model that you want deployed (for example: a pytorch model file)\n",
    "* code that you will be running in th service, that executes the model on a given input\n",
    "\n",
    "Azure Machine Learning allows you to separate the deployment into two separate components, so that you can keep the same code, but merely update the model. We define the mechanism by which you upload a model separately from your code as \"registering the model\".\n",
    "\n",
    "You can register a model by providing the local path of the model. You can provide the path of either a folder or a single file on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f490eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple file as the specific 'model' we're going to use.\n",
    "import pickle\n",
    "\n",
    "model_parameters = {'name': 'tutorial-model',\n",
    "        'parameters': {\n",
    "            'weights': [0.6, 0.3, 0.1]\n",
    "        }\n",
    "    }\n",
    "model_filename = 'tutorial_model.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as handle:\n",
    "    pickle.dump(model_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "467ac725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1}, {'id': 2}, {'id': 3}]\n"
     ]
    }
   ],
   "source": [
    "offers = sorted([{'id': 2}, {'id': 3}, {\"id\": 1} ], key=lambda o: o['id'])\n",
    "print(offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a11c494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model tutorial-model\n"
     ]
    }
   ],
   "source": [
    "# register the model\n",
    "model_properties = {\n",
    "    'source': 'tutorial'\n",
    "}\n",
    "model = Model.register(ws, \n",
    "                       model_name=model_parameters['name'], \n",
    "                       model_path=model_filename, \n",
    "                       tags=tags\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80acd40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(workspace=Workspace.create(name='data-workspace05a7960', subscription_id='651f2ed5-6e2f-41d0-b533-0cd28801ef2a', resource_group='data'), name=tutorial-model, id=tutorial-model:4, version=4, tags={}, properties={'source': 'tutorial'})\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ed24e",
   "metadata": {},
   "source": [
    "## 2. Prepare an entry script\n",
    "\n",
    "The entry script receives data submitted to a deployed web service and passes it to the model. It then returns the model's response to the client. The script is specific to your model. The entry script must understand the data that the model expects and returns.\n",
    "\n",
    "> You can use the environment variable `AZUREML_MODEL_DIR` to locate your model that you registered earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bee77bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source_dir/entry.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source_dir/entry.py \n",
    "\n",
    "# this is a very basic entry script\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    print('This is init')\n",
    "\n",
    "def run(data):\n",
    "    test = json.loads(data)\n",
    "    print(f'received data {test}')\n",
    "    return(f'test is {test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e45ced3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source_dir/entry.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source_dir/entry.py \n",
    "\n",
    "# this is an actual entry script\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "def init():\n",
    "    global parameters\n",
    "    with open(os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'tutorial_model.pkl'), 'rb') as handle:\n",
    "        parameters = pickle.load(handle)['parameters']\n",
    "\n",
    "# request should have: offers: [],  features: {}\n",
    "def run(request):\n",
    "    print(request)\n",
    "    recommendaton_request = json.loads(request)\n",
    "    # Run inference\n",
    "    recomendation = recommend(recommendaton_request)\n",
    "    print(recomendation)\n",
    "    return recomendation\n",
    "\n",
    "def recommend(recommendaton_request):\n",
    "    # sort the offers by ID\n",
    "    offers = sorted(recommendaton_request['offers'], key=lambda o: o['id'])\n",
    "    print(offers)\n",
    "    features = recommendaton_request['features']\n",
    "    return random.choices(offers, parameters['weights'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddcd07",
   "metadata": {},
   "source": [
    "## 3. Prepare an inference configuration and a deployment configuration\n",
    "\n",
    "### Inference\n",
    "\n",
    "An inference configuration describes the Docker container and files to use when initializing your web service. All of the files within your source directory, including subdirectories, will be zipped up and uploaded to the cloud when you deploy your web service.\n",
    "\n",
    "### Deployment\n",
    "\n",
    "A deployment configuration specifies the amount of memory and cores to reserve for your webservice will require in order to run, as well as configuration details of the underlying webservice. For example, a deployment configuration lets you specify that your service needs 2 gigabytes of memory, 2 CPU cores, 1 GPU core, and that you want to enable autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1f2fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_pip_requirements(name='tutorial_environment', file_path=\"./model_requirements.txt\")\n",
    "inference_config = InferenceConfig(environment=env, source_directory='source_dir', entry_script='./entry.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "addc0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a local webservice\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619ade",
   "metadata": {},
   "source": [
    "## 4. Deploy the model locally to ensure everything works\n",
    "\n",
    "This part needs docker installed and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b160b8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model tutorial-model:4 to /var/folders/r9/zky7_kgn5955p246_2p84brh0000gn/T/azureml_nhk5p_6w/tutorial-model/4\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry 145d1c48df42405ba082e5e8f747442e.azurecr.io\n",
      "Logging into Docker registry 145d1c48df42405ba082e5e8f747442e.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM 145d1c48df42405ba082e5e8f747442e.azurecr.io/azureml/azureml_984ae42c92ab7e15d7808a45a9ac459f\n",
      " ---> 3b27530f31be\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 2a82f9441e8c\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY1MWYyZWQ1LTZlMmYtNDFkMC1iNTMzLTBjZDI4ODAxZWYyYSIsInJlc291cmNlR3JvdXBOYW1lIjoiZGF0YSIsImFjY291bnROYW1lIjoiZGF0YS13b3Jrc3BhY2UwNWE3OTYwIiwid29ya3NwYWNlSWQiOiIxNDVkMWM0OC1kZjQyLTQwNWItYTA4Mi1lNWU4Zjc0NzQ0MmUifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 92812a98acb5\n",
      " ---> fc24f72280dd\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmptagh3v37.py' /var/azureml-app/main.py\n",
      " ---> Running in 19633732fa11\n",
      " ---> a5a54aa48e85\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 8af34ebe626d\n",
      " ---> 55fc23a3e196\n",
      "Successfully built 55fc23a3e196\n",
      "Successfully tagged tutorial-service-local:latest\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:5048fca18378f360dff024016afb4e5a3e064e82439dcaaafae4afa3fa34bbbb successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:6789\n",
      "2021-05-06T01:06:34,884960000+00:00 - rsyslog/run \n",
      "2021-05-06T01:06:34,885171000+00:00 - iot-server/run \n",
      "2021-05-06T01:06:34,885456100+00:00 - gunicorn/run \n",
      "2021-05-06T01:06:34,888445000+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-05-06T01:06:34,982888800+00:00 - iot-server/finish 1 0\n",
      "2021-05-06T01:06:34,984503900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (12)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 46\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-05-06 01:06:35,269 | root | INFO | Starting up app insights client\n",
      "2021-05-06 01:06:35,269 | root | INFO | Starting up request id generator\n",
      "2021-05-06 01:06:35,269 | root | INFO | Starting up app insight hooks\n",
      "2021-05-06 01:06:35,270 | root | INFO | Invoking user's init function\n",
      "2021-05-06 01:06:35,270 | root | INFO | Users's init has completed successfully\n",
      "2021-05-06 01:06:35,271 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-05-06 01:06:35,271 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-05-06 01:06:35,272 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service = Model.deploy(ws, \"tutorial-service-local\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe3e65",
   "metadata": {},
   "source": [
    "Call the local docker container to check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "253d7de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model responded with:\n",
      "[{'id': 1, 'price': 10, 'name': 'Bahn Mi'}]\n"
     ]
    }
   ],
   "source": [
    "# check the model works\n",
    "import requests\n",
    "import json\n",
    "\n",
    "uri = service.scoring_uri\n",
    "requests.get('http://localhost:6789')\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\"features\": {\n",
    "            \"country\": \"Australia\"\n",
    "        }, \"offers\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"price\": 10,\n",
    "                \"name\": \"Bahn Mi\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"price\": 20,\n",
    "                \"name\": \"Fish Stew\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"price\": 30,\n",
    "                \"name\": \"Steak Tartar\"\n",
    "            }\n",
    "        ]}\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print('model responded with:')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8666922",
   "metadata": {},
   "source": [
    "## 5. Choose a compute target.\n",
    "\n",
    "[Learn more about choosing a target](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python#choose-a-compute-target)\n",
    "\n",
    "Options are:\n",
    "* Local web service: for testing and debugging\n",
    "* Azure Kubernetes Services: High scale production (probably don't do this without bigger discussion)\n",
    "* Azure Container Instances: Low scale, less than 48GB RAM\n",
    "* Azure Machine Learning compute cluster: best for batch inferencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9861d",
   "metadata": {},
   "source": [
    "## 6. Re-deploy the model to the cloud\n",
    "\n",
    "Deploy to an Azure Container Instanec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "20d28459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(cpu_cores = 0.5, \n",
    "                                                           memory_gb = 1, \n",
    "                                                           tags=tags, \n",
    "                                                           auth_enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "724cd192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-05-06 11:27:44+10:00 Creating Container Registry if not exists.\n",
      "2021-05-06 11:27:44+10:00 Registering the environment.\n",
      "2021-05-06 11:27:45+10:00 Use the existing image.\n",
      "2021-05-06 11:27:45+10:00 Generating deployment configuration.\n",
      "2021-05-06 11:27:47+10:00 Submitting deployment to compute..\n",
      "2021-05-06 11:27:54+10:00 Checking the status of deployment tutorial-service-aci..\n",
      "2021-05-06 11:30:08+10:00 Checking the status of inference endpoint tutorial-service-aci.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "2021-05-06T01:29:59,991399800+00:00 - rsyslog/run \n",
      "2021-05-06T01:29:59,982298400+00:00 - iot-server/run \n",
      "2021-05-06T01:29:59,994369000+00:00 - gunicorn/run \n",
      "2021-05-06T01:30:00,158317300+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-05-06T01:30:00,721419900+00:00 - iot-server/finish 1 0\n",
      "2021-05-06T01:30:00,737985100+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (67)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 93\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up app insights client\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up request id generator\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up app insight hooks\n",
      "2021-05-06 01:30:01,602 | root | INFO | Invoking user's init function\n",
      "2021-05-06 01:30:01,602 | root | INFO | Users's init has completed successfully\n",
      "2021-05-06 01:30:01,606 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-05-06 01:30:01,607 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-05-06 01:30:01,610 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-05-06 01:30:08,577 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:08,577 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:08 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:14,368 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:14,369 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:14 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this cell creates a new ACI deployed into Azure.\n",
    "service = Model.deploy(ws, \n",
    "                       \"tutorial-service-aci\", \n",
    "                       [model], \n",
    "                       inference_config, \n",
    "                       aci_deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b37e1b",
   "metadata": {},
   "source": [
    "## 7. Test the resulting web service.\n",
    "\n",
    "When you deploy remotely, you may have key authentication enabled. The example below shows how to get your service key with Python in order to make an inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fcc080f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model responded:\n",
      "[{\"id\": 1, \"price\": 10, \"name\": \"Bahn Mi\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from azureml.core import Webservice\n",
    "\n",
    "service = Webservice(workspace=ws, name='tutorial-service-aci')\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key or token\n",
    "primary_key, _ = service.get_keys()\n",
    "\n",
    "# Set the appropriate headers\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "headers['Authorization'] = f'Bearer {primary_key}'\n",
    "\n",
    "# Make the request and display the response and logs\n",
    "data = {\"features\": {\n",
    "            \"country\": \"Australia\"\n",
    "        }, \"offers\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"price\": 10,\n",
    "                \"name\": \"Bahn Mi\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"price\": 20,\n",
    "                \"name\": \"Fish Stew\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"price\": 30,\n",
    "                \"name\": \"Steak Tartar\"\n",
    "            }\n",
    "        ]}\n",
    "\n",
    "data = json.dumps(data)\n",
    "resp = requests.post(scoring_uri, data=data, headers=headers)\n",
    "print('the model responded:')\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6f969",
   "metadata": {},
   "source": [
    "You can get the logs from the remote ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "650ee75e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06T01:29:59,991399800+00:00 - rsyslog/run \n",
      "2021-05-06T01:29:59,982298400+00:00 - iot-server/run \n",
      "2021-05-06T01:29:59,994369000+00:00 - gunicorn/run \n",
      "2021-05-06T01:30:00,158317300+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_abe832532d68e7634ad48db5ac241baf/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-05-06T01:30:00,721419900+00:00 - iot-server/finish 1 0\n",
      "2021-05-06T01:30:00,737985100+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (67)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 93\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up app insights client\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up request id generator\n",
      "2021-05-06 01:30:01,601 | root | INFO | Starting up app insight hooks\n",
      "2021-05-06 01:30:01,602 | root | INFO | Invoking user's init function\n",
      "2021-05-06 01:30:01,602 | root | INFO | Users's init has completed successfully\n",
      "2021-05-06 01:30:01,606 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-05-06 01:30:01,607 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-05-06 01:30:01,610 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-05-06 01:30:08,577 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:08,577 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:08 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:14,368 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:14,369 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:14 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:16,163 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:16,163 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:16 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:25,248 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:25,249 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:25 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:26,209 | root | INFO | Validation Request Content-Type\n",
      "2021-05-06 01:30:26,209 | root | INFO | Scoring Timer is set to 60.0 seconds\n",
      "{\"features\": {\"country\": \"Australia\"}, \"offers\": [{\"id\": 1, \"price\": 10, \"name\": \"Bahn Mi\"}, {\"id\": 2, \"price\": 20, \"name\": \"Fish Stew\"}, {\"id\": 3, \"price\": 30, \"name\": \"Steak Tartar\"}]}\n",
      "[{'id': 1, 'price': 10, 'name': 'Bahn Mi'}, {'id': 2, 'price': 20, 'name': 'Fish Stew'}, {'id': 3, 'price': 30, 'name': 'Steak Tartar'}]\n",
      "[{'id': 1, 'price': 10, 'name': 'Bahn Mi'}]\n",
      "2021-05-06 01:30:26,210 | root | INFO | 200\n",
      "127.0.0.1 - - [06/May/2021:01:30:26 +0000] \"POST /score HTTP/1.0\" 200 43 \"-\" \"python-requests/2.25.1\"\n",
      "2021-05-06 01:30:45,296 | root | INFO | Swagger file not present\n",
      "2021-05-06 01:30:45,296 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:01:30:45 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 01:30:45,747 | root | INFO | Validation Request Content-Type\n",
      "2021-05-06 01:30:45,748 | root | INFO | Scoring Timer is set to 60.0 seconds\n",
      "{\"features\": {\"country\": \"Australia\"}, \"offers\": [{\"id\": 1, \"price\": 10, \"name\": \"Bahn Mi\"}, {\"id\": 2, \"price\": 20, \"name\": \"Fish Stew\"}, {\"id\": 3, \"price\": 30, \"name\": \"Steak Tartar\"}]}\n",
      "[{'id': 1, 'price': 10, 'name': 'Bahn Mi'}, {'id': 2, 'price': 20, 'name': 'Fish Stew'}, {'id': 3, 'price': 30, 'name': 'Steak Tartar'}]\n",
      "[{'id': 1, 'price': 10, 'name': 'Bahn Mi'}]\n",
      "2021-05-06 01:30:45,749 | root | INFO | 200\n",
      "127.0.0.1 - - [06/May/2021:01:30:45 +0000] \"POST /score HTTP/1.0\" 200 43 \"-\" \"python-requests/2.25.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ff3d3",
   "metadata": {},
   "source": [
    "## WELL DONE!\n",
    "\n",
    "You deployed your first model!\n",
    "\n",
    "The next part is making it intelligent.\n",
    "\n",
    "But first, clean up the resources you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c7d6548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No service with name tutorial-service-aci found to delete.\n"
     ]
    }
   ],
   "source": [
    "# delete the ACI service you created\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc80d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all the models we created\n",
    "models = Model.list(ws, name=model_parameters['name'])\n",
    "for m in models:\n",
    "    print('deleting', model.name, 'version:' , model.version)\n",
    "    m.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9354ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
